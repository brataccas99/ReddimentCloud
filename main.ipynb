{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## COLLECTING REAL DATA AND CREATING CASE STUDY DATASET##################### \n",
    "import pandas\n",
    "import praw\n",
    "from dotenv import dotenv_values\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "# Authenticate with Reddit using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=env[\"CLIENT_ID\"],\n",
    "    client_secret=env[\"CLIENT_SECRET\"],\n",
    "    user_agent=env[\"USER_AGENT\"],\n",
    "    redirect_uri=env[\"REDIRECT_URI\"],\n",
    "    refresh_token=env[\"REFRESH_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "csv_file_name = \"reddit_posts_with_comments.csv\"\n",
    "if os.path.exists(csv_file_name):\n",
    "    print(\"CSV file already exists. Appending new data and avoiding duplicates.\")\n",
    "    df = pandas.read_csv(csv_file_name)  # Read existing CSV into a DataFrame\n",
    "else:\n",
    "    print(\"CSV file does not exist. It will be created after fetching new data.\")\n",
    "    df = pandas.DataFrame(columns=[\"Title\", \"Id\", \"Comments\"])\n",
    "\n",
    "# Create a subreddit instance\n",
    "targetObjects = ['conspiracy',\n",
    "                 'WhitePeopleTwitter', 'politics', 'Republican', 'worldnews', 'CombatFootage', 'UkraineRussiaReport']\n",
    "for subreddit_name in targetObjects:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Print subreddit name\n",
    "    print(subreddit.display_name)\n",
    "\n",
    "    # Lists to store submission information\n",
    "    titles = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    comments = []\n",
    "\n",
    "    # Loop through the newest 21 submissions in the subreddit\n",
    "    for iteration, submission in enumerate(subreddit.hot(limit=20)):\n",
    "        print(f\"post {iteration}/20\")\n",
    "        # Check if the submission ID already exists in the DataFrame to avoid duplication\n",
    "        if submission.id not in df[\"Id\"].values:\n",
    "            # Add submission title to the titles list\n",
    "            titles.append(submission.title)\n",
    "            ids.append(submission.id)  # Add submission ID to the ids list\n",
    "\n",
    "            # Fetch comments for the current submission\n",
    "            submission.comments.replace_more(limit=40)\n",
    "            submission_comments = []\n",
    "            for comment in submission.comments.list():\n",
    "                # Check if the comment author's username contains \"bot\"\n",
    "                if 'bot' not in comment.name:\n",
    "                    # Use BeautifulSoup to remove HTML tags from content\n",
    "                    soup = BeautifulSoup(comment.body, 'html.parser')\n",
    "                    filtered_content = soup.get_text()\n",
    "\n",
    "                    # Remove URLs from filtered_content\n",
    "                    filtered_content = re.sub(\n",
    "                        r'http\\S+|www\\S+', '', filtered_content)\n",
    "\n",
    "                    # Remove only #\n",
    "                    filtered_content = re.sub(r'#', '', filtered_content).lower()\n",
    "                    submission_comments.append(filtered_content)\n",
    "            comments.append(submission_comments)\n",
    "\n",
    "        # Create a DataFrame with the new data\n",
    "        new_data = pandas.DataFrame(\n",
    "            {\"Title\": titles, \"Id\": ids, \"Comments\": comments}\n",
    "        )\n",
    "\n",
    "        # Append/concat the new data to the existing DataFrame\n",
    "        df = pandas.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates based on the 'Id' column (submission IDs)\n",
    "        df.drop_duplicates(subset=\"Id\", keep=\"last\", inplace=True)\n",
    "    # Save the DataFrame to the CSV file\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"CSV file '{csv_file_name}' has been generated/updated with the new Reddit posts and comments while avoiding duplicates.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############CLEANING THE DATASET ####################\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Read the CSV file\n",
    "input_csv = \"reddit_posts_with_comments.csv\"\n",
    "output_csv = \"cleaned_reddit_posts.csv\"\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Function to find and remove emojis from a string\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# Function to clean and filter a single comment\n",
    "def clean_and_filter_comment(comment):\n",
    "    # Convert the string representation of the list to an actual list\n",
    "    comment_list = ast.literal_eval(comment)\n",
    "    # Filter out comments that match the specified patterns\n",
    "    cleaned_comments = []\n",
    "    for c in comment_list:\n",
    "        # Remove '[deleted]' comments\n",
    "        if c.strip() != \"[deleted]\":\n",
    "            # Remove emojis from the comment\n",
    "            c = remove_emojis(c)\n",
    "            # Remove links\n",
    "            c = re.sub(r'http[s]?://\\S+', '', c)\n",
    "            # Remove the \"[meta]\" pattern (case-insensitive)\n",
    "            if not re.search(r'\\[meta\\]', c, re.IGNORECASE):\n",
    "                # Remove extra spaces and append the cleaned comment\n",
    "                cleaned_comments.append(re.sub(r'\\s+', ' ', c.strip()))\n",
    "    return cleaned_comments\n",
    "\n",
    "# Apply the cleaning and filtering function to the 'Comments' column\n",
    "df['Comments'] = df['Comments'].apply(clean_and_filter_comment)\n",
    "\n",
    "# Remove rows where all comments were filtered out\n",
    "df = df[df['Comments'].apply(len) > 0]\n",
    "\n",
    "# Save the cleaned and filtered DataFrame to a new CSV file\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TOKENIZING THE TARGET DATASET ##################### \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "csv_tokenized = \"tokenized_csv.csv\"\n",
    "csv_input = \"cleaned_reddit_posts.csv\"\n",
    "df = pd.read_csv(csv_input)\n",
    "\n",
    "# Function to clean the text using regex\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', str(text))\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "# Clean the 'Comments' column\n",
    "df['Comments'] = df['Comments'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokenized_text'] = df['Comments'].apply(word_tokenize)\n",
    "\n",
    "# Removal of stopwords\n",
    "stopwords_english = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if token not in stopwords_english])\n",
    "\n",
    "# Remove tokens with a single character\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if len(token) > 1])\n",
    "\n",
    "# Drop the unnecessary columns (keep only the 'tokenized_text' column)\n",
    "df_cleaned = df[['tokenized_text']]\n",
    "\n",
    "# Save the cleaned DataFrame to the CSV file\n",
    "df_cleaned.to_csv(csv_tokenized, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df_cleaned.shape)\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\n",
    "    f\"CSV file '{csv_tokenized}' has been generated/updated with the tokenized text while avoiding duplicates and cleaning the data.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### DOWNLOADING AND CACHING MODELS ################################\n",
    "import logging\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define a mapping of languages to model names\n",
    "language_to_model = {\n",
    "    'en': \"IMSyPP/hate_speech_en\",\n",
    "    'it': \"IMSyPP/hate_speech_it\",\n",
    "    'nl': \"IMSyPP/hate_speech_nl\",\n",
    "    'sl': \"IMSyPP/hate_speech_slo\",\n",
    "}\n",
    "\n",
    "# Function to download models to the cache directory\n",
    "def download_models_to_cache():\n",
    "    # Specify the cache directory for local caching\n",
    "    cache_dir = \".cache\"\n",
    "    # Loop over the models and download them to the cache\n",
    "    for model_name in language_to_model.values():\n",
    "        try:\n",
    "            logger.info(f\"Downloading and caching model '{model_name}'...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "            logger.info(f\"Model '{model_name}' downloaded and cached successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while downloading the model '{model_name}': {e}\")\n",
    "\n",
    "# Call the function to download models to the cache\n",
    "download_models_to_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## PROCESSING DATA AND CREATING THE SENTIMENT ANALYSIS ####################\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Set the logging level for the transformers library to ERROR\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the Reddit posts CSV file\n",
    "input_csv = \"cleaned_reddit_posts.csv\"\n",
    "output_csv = \"reddit_posts_with_labels.csv\"\n",
    "\n",
    "# Define a mapping of languages to model names\n",
    "language_to_model = {\n",
    "    'en': \"IMSyPP/hate_speech_en\",\n",
    "    'it': \"IMSyPP/hate_speech_it\",\n",
    "    'nl': \"IMSyPP/hate_speech_nl\",\n",
    "    'sl': \"IMSyPP/hate_speech_slo\",\n",
    "}\n",
    "\n",
    "# Define the default model for cases where language detection fails\n",
    "default_model_name = \"IMSyPP/hate_speech_en\"\n",
    "\n",
    "def load_model(language):\n",
    "    model_name = language_to_model.get(language, default_model_name)\n",
    "    cache_dir = \".cache\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    \n",
    "    # Set special tokens (if applicable)\n",
    "    special_tokens = {}\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens[\"bos_token\"] = \"[BOS]\"  # Use the string token here\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens[\"eos_token\"] = \"[EOS]\"  # Use the string token here\n",
    "    \n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to analyze a comment and return the results\n",
    "def analyze_comment(comment, language, tokenizer, model):\n",
    "    try:\n",
    "        inputs = tokenizer(comment, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "        analyze_error = None\n",
    "    except Exception as e:\n",
    "        probabilities = [0.0] * 4\n",
    "        analyze_error = str(e)\n",
    "    return probabilities, language, analyze_error\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Total number of comments to process\n",
    "total_comments = df['Comments'].apply(len).sum()\n",
    "\n",
    "# Batch size for writing results to CSV\n",
    "batch_size = 100\n",
    "\n",
    "# Processed comment count\n",
    "processed_comments = 0\n",
    "processed_batch_count = 0\n",
    "\n",
    "# Initialize lists to store final results\n",
    "final_results = []\n",
    "\n",
    "# Load the default model once for cases where language detection fails\n",
    "default_tokenizer, default_model = load_model('en')\n",
    "\n",
    "# Iterate over rows in the CSV\n",
    "progress_bar = tqdm(total=total_comments, desc=\"Processing Comments\", leave=False)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    comment_list = eval(row['Comments'])  # Assuming the comments are in a list format\n",
    "\n",
    "    # Split the comment list into batches\n",
    "    comment_batches = [comment_list[i:i + batch_size] for i in range(0, len(comment_list), batch_size)]\n",
    "\n",
    "    for batch in comment_batches:\n",
    "        try:\n",
    "            # Filter out empty or very short comments\n",
    "            batch = [comment for comment in batch if len(comment.strip()) > 7]  # Adjust the length threshold as needed\n",
    "            \n",
    "            if not batch:\n",
    "                continue  # Skip the batch if no valid comments are present\n",
    "            \n",
    "            # Detect language for the first comment in the batch\n",
    "            language = detect(batch[0])\n",
    "            \n",
    "            # Load the model and tokenizer for the detected language\n",
    "            tokenizer, model = load_model(language)\n",
    "            \n",
    "            # Process the batch of comments sequentially\n",
    "            batch_texts = batch  # Store the batch of comments as a list of strings\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            probabilities = torch.softmax(outputs.logits, dim=1).tolist()\n",
    "            analyze_error = None\n",
    "\n",
    "            # Process the results for each comment in the batch\n",
    "            for i, comment in enumerate(batch_texts):\n",
    "                result_row = (comment,f\"{probabilities[i][0] * 100:.2f}%\",f\"{probabilities[i][1] * 100:.2f}%\",f\"{probabilities[i][2] * 100:.2f}%\",f\"{probabilities[i][3] * 100:.2f}%\",language,analyze_error if analyze_error is not None else \"None\")\n",
    "                final_results.append(result_row)\n",
    "\n",
    "                # Update processed comment count\n",
    "                processed_comments += 1\n",
    "\n",
    "                # Save results to CSV every batch_size comments\n",
    "                if processed_comments % batch_size == 0:\n",
    "                    result_df = pd.DataFrame(final_results, columns=['comment','probabilities_acceptable', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent', 'language', 'errors'])\n",
    "                    result_df.to_csv(output_csv, index=False)\n",
    "\n",
    "                # Update progress description and postfix\n",
    "                progress_percent = (processed_comments / total_comments) * 100\n",
    "                progress_bar.set_description(f\"Processing Comments - {progress_percent:.2f}%\")\n",
    "                progress_bar.set_postfix({\"Processed\": f\"{processed_comments}/{total_comments}\"})\n",
    "            \n",
    "            # Update progress batch \n",
    "            processed_batch_count += 1\n",
    "            progress_batch = (processed_batch_count / len(comment_batches)) * 100\n",
    "            progress_bar.set_description(f\"Processing Batches - {progress_batch:.2f}%\")\n",
    "            progress_bar.set_postfix({\"Processed\": f\"{processed_batch_count}/{len(comment_batches)}\"})\n",
    "            \n",
    "            result_df = pd.DataFrame(final_results, columns=['comment','probabilities_acceptable', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent', 'language', 'errors'])\n",
    "            result_df.to_csv(output_csv, index=False, mode='a')\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error processing batch: %s\", str(e))\n",
    "\n",
    "# Save the final results\n",
    "result_df = pd.DataFrame(final_results, columns=['comment','probabilities_acceptable', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent', 'language', 'errors'])\n",
    "result_df.to_csv(output_csv, index=False, mode='a')\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "logger.info(\"Analysis completed. Results saved to: %s\", output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import os\n",
    "from azure.servicebus.aio import ServiceBusClient\n",
    "from azure.servicebus import ServiceBusMessage\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "#### send the images to the queue\n",
    "async def send_single_message(sender, data):\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    message_body = json.dumps(data)\n",
    "\n",
    "    # Create a Service Bus message and send it to the queue\n",
    "    message = ServiceBusMessage(message_body)\n",
    "    await sender.send_messages(message)\n",
    "    print(\"Sent a single message\")\n",
    "\n",
    "\n",
    "namespace_connection_str = env[\"NAMESPACE_CONNECTION_STR\"]\n",
    "queue_name = env[\"QUEUE_NAME\"]\n",
    "servicebus_client = ServiceBusClient.from_connection_string(\n",
    "    conn_str=namespace_connection_str\n",
    ")\n",
    "\n",
    "# Get the queue sender\n",
    "sender = servicebus_client.get_queue_sender(queue_name=queue_name)\n",
    "\n",
    "# Load the CSV data into a pandas DataFrame\n",
    "csv_path = \"reddit_posts_with_labels.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Clean the data by removing rows with non-numeric values\n",
    "data = data[data[\"probabilities_acceptable\"].str.match(r\"^\\d+\\.\\d+%$\")]\n",
    "\n",
    "# Convert percentage strings to numeric values\n",
    "data[\"probabilities_acceptable\"] = pd.to_numeric(\n",
    "    data[\"probabilities_acceptable\"].str.rstrip(\"%\")\n",
    ")\n",
    "data[\"probabilities_hate\"] = data[\"probabilities_hate\"].str.rstrip(\"%\").astype(float)\n",
    "data[\"probabilities_offensive\"] = (\n",
    "    data[\"probabilities_offensive\"].str.rstrip(\"%\").astype(float)\n",
    ")\n",
    "data[\"probabilities_violent\"] = (\n",
    "    data[\"probabilities_violent\"].str.rstrip(\"%\").astype(float)\n",
    ")\n",
    "\n",
    "# Calculate average probabilities for each content type\n",
    "avg_accept = data[\"probabilities_acceptable\"].mean()\n",
    "avg_hate = data[\"probabilities_hate\"].mean()\n",
    "avg_offensive = data[\"probabilities_offensive\"].mean()\n",
    "avg_violent = data[\"probabilities_violent\"].mean()\n",
    "\n",
    "# Create a bar graph using Matplotlib\n",
    "labels = [\"Acceptable\", \"Hate\", \"Offensive\", \"Violent\"]\n",
    "values = [avg_accept, avg_hate, avg_offensive, avg_violent]\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.ylabel(\"Average Probability\")\n",
    "plt.title(\"Average Probabilities of Content Types\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the bar graph as an image\n",
    "image_path = \"bar_graph.png\"\n",
    "plt.savefig(image_path)\n",
    "plt.close()\n",
    "\n",
    "# Read the image as binary data and encode it as base64\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    image_binary = image_file.read()\n",
    "    image_base64 = base64.b64encode(image_binary).decode(\"utf-8\")\n",
    "\n",
    "# Create a dictionary to store image data\n",
    "image_data = {\"chart_type\": \"bargraphs\", \"image_base64\": image_base64}\n",
    "\n",
    "# Remove the temporary image file\n",
    "os.remove(image_path)\n",
    "await send_single_message(sender, image_data)\n",
    "\n",
    "# Close the sender and the client\n",
    "await sender.close()\n",
    "await servicebus_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import os\n",
    "from azure.servicebus.aio import ServiceBusClient\n",
    "from azure.servicebus import ServiceBusMessage\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "# Function to send a single message\n",
    "async def send_single_message(sender, data):\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    message_body = json.dumps(data)\n",
    "\n",
    "    # Create a Service Bus message and send it to the queue\n",
    "    message = ServiceBusMessage(message_body)\n",
    "    await sender.send_messages(message)\n",
    "    print(\"Sent a single message\")\n",
    "\n",
    "\n",
    "namespace_connection_str = env[\"NAMESPACE_CONNECTION_STR\"]\n",
    "queue_name = env[\"QUEUE_NAME\"]\n",
    "servicebus_client = ServiceBusClient.from_connection_string(\n",
    "    conn_str=namespace_connection_str\n",
    ")\n",
    "\n",
    "# Get the queue sender\n",
    "sender = servicebus_client.get_queue_sender(queue_name=queue_name)\n",
    "\n",
    "# Read the CSV file\n",
    "dataset = pd.read_csv(\"reddit_posts_with_labels.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Clean the data by removing rows with non-numeric values\n",
    "dataset = dataset[dataset[\"probabilities_acceptable\"].str.match(r\"^\\d+\\.\\d+%$\")]\n",
    "\n",
    "# Convert probability columns to numeric (float) type\n",
    "probability_columns = [\n",
    "    \"probabilities_acceptable\",\n",
    "    \"probabilities_hate\",\n",
    "    \"probabilities_offensive\",\n",
    "    \"probabilities_violent\",\n",
    "]\n",
    "for column in probability_columns:\n",
    "    dataset[column] = pd.to_numeric(dataset[column].str.rstrip(\"%\"))\n",
    "\n",
    "# Create a histogram plot\n",
    "plt.hist(dataset[\"probabilities_acceptable\"], bins=20, alpha=0.5, label=\"Acceptable\")\n",
    "plt.hist(dataset[\"probabilities_hate\"], bins=20, alpha=0.5, label=\"Hate\")\n",
    "plt.hist(dataset[\"probabilities_offensive\"], bins=20, alpha=0.5, label=\"Offensive\")\n",
    "plt.hist(dataset[\"probabilities_violent\"], bins=20, alpha=0.5, label=\"Violent\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Probabilities\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the histogram plot as an image\n",
    "image_path = \"histogram.png\"\n",
    "plt.savefig(image_path)\n",
    "plt.close()\n",
    "\n",
    "# Read the image as binary data and encode it as base64\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    image_binary = image_file.read()\n",
    "    image_base64 = base64.b64encode(image_binary).decode(\"utf-8\")\n",
    "\n",
    "# Create a dictionary to store image data\n",
    "image_data = {\"chart_type\": \"histograms\", \"image_base64\": image_base64}\n",
    "\n",
    "# Remove the temporary image file\n",
    "os.remove(image_path)\n",
    "\n",
    "# Send the image data to the queue\n",
    "await send_single_message(sender, image_data)\n",
    "\n",
    "# Close the sender and the client\n",
    "await sender.close()\n",
    "await servicebus_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import os\n",
    "from azure.servicebus.aio import ServiceBusClient\n",
    "from azure.servicebus import ServiceBusMessage\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "# Function to send a single message\n",
    "async def send_single_message(sender, data):\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    message_body = json.dumps(data)\n",
    "\n",
    "    # Create a Service Bus message and send it to the queue\n",
    "    message = ServiceBusMessage(message_body)\n",
    "    await sender.send_messages(message)\n",
    "    print(\"Sent a single message\")\n",
    "\n",
    "\n",
    "namespace_connection_str = env[\"NAMESPACE_CONNECTION_STR\"]\n",
    "queue_name = env[\"QUEUE_NAME\"]\n",
    "servicebus_client = ServiceBusClient.from_connection_string(\n",
    "    conn_str=namespace_connection_str\n",
    ")\n",
    "\n",
    "# Get the queue sender\n",
    "sender = servicebus_client.get_queue_sender(queue_name=queue_name)\n",
    "\n",
    "# Read the CSV file\n",
    "dataset = pd.read_csv(\"reddit_posts_with_labels.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Clean the data by removing rows with non-numeric values\n",
    "dataset = dataset[dataset[\"probabilities_acceptable\"].str.match(r\"^\\d+\\.\\d+%$\")]\n",
    "\n",
    "# Convert probability columns to numeric (float) type\n",
    "probability_columns = [\n",
    "    \"probabilities_acceptable\",\n",
    "    \"probabilities_hate\",\n",
    "    \"probabilities_offensive\",\n",
    "    \"probabilities_violent\",\n",
    "]\n",
    "for column in probability_columns:\n",
    "    dataset[column] = pd.to_numeric(dataset[column].str.rstrip(\"%\"))\n",
    "\n",
    "# Create a scatter plot\n",
    "scatter = plt.scatter(\n",
    "    dataset[\"probabilities_acceptable\"],\n",
    "    dataset[\"probabilities_hate\"],\n",
    "    c=dataset[\"probabilities_violent\"],  # Use 'probabilities_violent' for color\n",
    "    cmap=\"viridis\",\n",
    "    s=dataset[\"probabilities_offensive\"]\n",
    "    * 100,  # Use 'probabilities_offensive' for marker size\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Acceptable Probability\")\n",
    "plt.ylabel(\"Hate Probability\")\n",
    "plt.title(\"Scatter Plot of Hate vs. Acceptable Probabilities\")\n",
    "plt.colorbar(label=\"Violent Probability\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the scatter plot as an image\n",
    "image_path = \"scatterplot.png\"\n",
    "plt.savefig(image_path)\n",
    "plt.close()\n",
    "\n",
    "# Read the image as binary data and encode it as base64\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    image_binary = image_file.read()\n",
    "    image_base64 = base64.b64encode(image_binary).decode(\"utf-8\")\n",
    "\n",
    "# Create a dictionary to store image data\n",
    "image_data = {\"chart_type\": \"scatterplots\", \"image_base64\": image_base64}\n",
    "\n",
    "# Remove the temporary image file\n",
    "os.remove(image_path)\n",
    "\n",
    "# Send the image data to the queue\n",
    "await send_single_message(sender, image_data)\n",
    "\n",
    "# Close the sender and the client\n",
    "await sender.close()\n",
    "await servicebus_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import os\n",
    "from azure.servicebus.aio import ServiceBusClient\n",
    "from azure.servicebus import ServiceBusMessage\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "# Function to send a single message\n",
    "async def send_single_message(sender, data):\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    message_body = json.dumps(data)\n",
    "\n",
    "    # Create a Service Bus message and send it to the queue\n",
    "    message = ServiceBusMessage(message_body)\n",
    "    await sender.send_messages(message)\n",
    "    print(\"Sent a single message\")\n",
    "\n",
    "\n",
    "namespace_connection_str = env[\"NAMESPACE_CONNECTION_STR\"]\n",
    "queue_name = env[\"QUEUE_NAME\"]\n",
    "servicebus_client = ServiceBusClient.from_connection_string(\n",
    "    conn_str=namespace_connection_str\n",
    ")\n",
    "\n",
    "# Get the queue sender\n",
    "sender = servicebus_client.get_queue_sender(queue_name=queue_name)\n",
    "\n",
    "# Read the CSV file\n",
    "dataset = pd.read_csv(\"reddit_posts_with_labels.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Clean the data by removing rows with non-numeric values\n",
    "dataset = dataset[dataset[\"probabilities_acceptable\"].str.match(r\"^\\d+\\.\\d+%$\")]\n",
    "\n",
    "# Convert probability columns to numeric (float) type\n",
    "probability_columns = [\n",
    "    \"probabilities_acceptable\",\n",
    "    \"probabilities_hate\",\n",
    "    \"probabilities_offensive\",\n",
    "    \"probabilities_violent\",\n",
    "]\n",
    "for column in probability_columns:\n",
    "    dataset[column] = pd.to_numeric(dataset[column].str.rstrip(\"%\"))\n",
    "\n",
    "# Create and save the bar chart as an image\n",
    "data = dataset[\n",
    "    [\n",
    "        \"probabilities_acceptable\",\n",
    "        \"probabilities_hate\",\n",
    "        \"probabilities_offensive\",\n",
    "        \"probabilities_violent\",\n",
    "    ]\n",
    "]\n",
    "ax = data.plot(kind=\"bar\", stacked=True)\n",
    "plt.xlabel(\"Comment\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Content Type Probabilities in Each Comment\")\n",
    "plt.legend(title=\"Content Type\", loc=\"upper right\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "image_path = \"barchart.png\"\n",
    "plt.savefig(image_path)\n",
    "plt.close()\n",
    "\n",
    "# Read the image as binary data and encode it as base64\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    image_binary = image_file.read()\n",
    "    image_base64 = base64.b64encode(image_binary).decode(\"utf-8\")\n",
    "\n",
    "# Create a dictionary to store image data\n",
    "image_data = {\"chart_type\": \"barcharts\", \"image_base64\": image_base64}\n",
    "\n",
    "# Remove the temporary image file\n",
    "os.remove(image_path)\n",
    "\n",
    "# Send the image data to the queue\n",
    "await send_single_message(sender, image_data)\n",
    "\n",
    "# Close the sender and the client\n",
    "await sender.close()\n",
    "await servicebus_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import base64\n",
    "import os\n",
    "from azure.servicebus.aio import ServiceBusClient\n",
    "from azure.servicebus import ServiceBusMessage\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "# Function to send a single message\n",
    "async def send_single_message(sender, data):\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    message_body = json.dumps(data)\n",
    "\n",
    "    # Create a Service Bus message and send it to the queue\n",
    "    message = ServiceBusMessage(message_body)\n",
    "    await sender.send_messages(message)\n",
    "    print(\"Sent \", message_body)\n",
    "\n",
    "\n",
    "# Function to compress an image and return base64 encoded string\n",
    "def compress_and_encode_image(image_path):\n",
    "    # Open the image using Pillow (PIL)\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image = Image.open(image_file)\n",
    "\n",
    "        # Resize the image to a lower resolution\n",
    "        image = image.resize((400, 250))\n",
    "\n",
    "        # Compress the image by saving it to BytesIO with a lower quality\n",
    "        compressed_image = io.BytesIO()\n",
    "        image.save(compressed_image, format=\"PNG\", quality=30)\n",
    "\n",
    "        # Encode the compressed image as base64\n",
    "        compressed_image_base64 = base64.b64encode(compressed_image.getvalue()).decode(\n",
    "            \"utf-8\"\n",
    "        )\n",
    "\n",
    "    return compressed_image_base64\n",
    "\n",
    "\n",
    "namespace_connection_str = env[\"NAMESPACE_CONNECTION_STR\"]\n",
    "queue_name = env[\"QUEUE_NAME\"]\n",
    "servicebus_client = ServiceBusClient.from_connection_string(\n",
    "    conn_str=namespace_connection_str\n",
    ")\n",
    "\n",
    "# Get the queue sender\n",
    "sender = servicebus_client.get_queue_sender(queue_name=queue_name)\n",
    "\n",
    "# Read the CSV file\n",
    "dataset = pd.read_csv(\"reddit_posts_with_labels.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Clean the data by removing rows with non-numeric values\n",
    "dataset = dataset[dataset[\"probabilities_acceptable\"].str.match(r\"^\\d+\\.\\d+%$\")]\n",
    "\n",
    "# Convert probability columns to numeric (float) type\n",
    "probability_columns = [\n",
    "    \"probabilities_acceptable\",\n",
    "    \"probabilities_hate\",\n",
    "    \"probabilities_offensive\",\n",
    "    \"probabilities_violent\",\n",
    "]\n",
    "\n",
    "for column in probability_columns:\n",
    "    dataset[column] = pd.to_numeric(dataset[column].str.rstrip(\"%\"))\n",
    "\n",
    "# Generate word clouds for each probability column\n",
    "for column in probability_columns:\n",
    "    filtered_comments = \" \".join(\n",
    "        [\n",
    "            text\n",
    "            for text, prob_value in zip(dataset[\"comment\"], dataset[column])\n",
    "            if prob_value >= 10\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Check if there are words to generate a word cloud\n",
    "    if filtered_comments:\n",
    "        # Generate and display the word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, height=500, random_state=21, max_font_size=110\n",
    "        ).generate(filtered_comments)\n",
    "\n",
    "        # Save the word cloud as an image\n",
    "        image_path = f\"wordcloud_{column}.png\"\n",
    "        wordcloud.to_file(image_path)\n",
    "\n",
    "        # Compress the image and encode it as base64\n",
    "        compressed_image_base64 = compress_and_encode_image(image_path)\n",
    "\n",
    "        # Create a dictionary to store compressed image data\n",
    "        image_data = {\n",
    "            \"chart_type\": f\"wordcloudimages - {column}\",\n",
    "            \"image_base64\": compressed_image_base64,\n",
    "        }\n",
    "\n",
    "        # Send the compressed image data to the queue\n",
    "        await send_single_message(sender, image_data)\n",
    "\n",
    "        # Remove the temporary image file\n",
    "        os.remove(image_path)\n",
    "    else:\n",
    "        print(f\"No words found for {column}\")\n",
    "\n",
    "# Close the sender and the client\n",
    "await sender.close()\n",
    "await servicebus_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.servicebus.aio import ServiceBusClient\n",
    "from azure.servicebus import ServiceBusMessage\n",
    "from dotenv import dotenv_values\n",
    "import csv\n",
    "import json\n",
    "\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "async def send_single_message(sender, data):\n",
    "    # Convert the list of dictionaries to a JSON-formatted string\n",
    "    message_body = json.dumps(data)\n",
    "    truncated_data = message_body\n",
    "    # Check if the data exceeds the allowed limit\n",
    "    if len(message_body) > 260000:\n",
    "        # If it exceeds, truncate the data while ensuring it is valid JSON\n",
    "        truncated_data = truncate_and_ensure_valid_json(message_body, 260000)\n",
    "        print(\"Data truncated to fit within the message size limit.\")\n",
    "\n",
    "    # Create a Service Bus message and send it to the queue\n",
    "    message = ServiceBusMessage(truncated_data)\n",
    "    await sender.send_messages(message)\n",
    "\n",
    "    print(\"Sent message\", message)\n",
    "\n",
    "\n",
    "def truncate_and_ensure_valid_json(data, max_size):\n",
    "    # Find the last occurrence of '}' before the max_size\n",
    "    last_close_brace_index = data.rfind(\"}\", 0, max_size)\n",
    "\n",
    "    # Truncate to the position before the last '}'\n",
    "    truncated_data = data[: last_close_brace_index + 1]\n",
    "\n",
    "    # Ensure that the JSON ends with '}]}' if it doesn't already\n",
    "    if not truncated_data.endswith(\"}]}\"):\n",
    "        truncated_data += \"]}\"\n",
    "\n",
    "    return truncated_data\n",
    "\n",
    "\n",
    "namespace_connection_str = env[\"NAMESPACE_CONNECTION_STR\"]\n",
    "queue_name = env[\"QUEUE_NAME\"]\n",
    "servicebus_client = ServiceBusClient.from_connection_string(\n",
    "    conn_str=namespace_connection_str\n",
    ")\n",
    "\n",
    "# Get the queue sender\n",
    "sender = servicebus_client.get_queue_sender(queue_name=queue_name)\n",
    "\n",
    "# Read CSV file and convert to a list of dictionaries\n",
    "csv_file = \"./reddit_posts_with_labels.csv\"\n",
    "json_data = []\n",
    "\n",
    "with open(csv_file, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    csv_reader = csv.DictReader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        json_data.append(row)\n",
    "\n",
    "# Remove the \"_id\" field from the JSON documents\n",
    "for doc in json_data:\n",
    "    doc.pop(\"_id\", None)\n",
    "\n",
    "# Write the list of dictionaries to a JSON file\n",
    "json_file = \"data.json\"\n",
    "\n",
    "\n",
    "with open(json_file, \"w\") as jsonfile:\n",
    "    json.dump(json_data, jsonfile, indent=4)\n",
    "\n",
    "print(\"CSV data converted to JSON successfully.\")\n",
    "\n",
    "# Load the JSON data from the file\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    data_to_insert = json.load(file)\n",
    "\n",
    "# Check for duplicates in the JSON data based on the \"comment\" field and remove them\n",
    "seen_comments = set()\n",
    "unique_data_to_insert = []\n",
    "\n",
    "for doc in data_to_insert:\n",
    "    comment = doc[\"comment\"]\n",
    "    if comment not in seen_comments:\n",
    "        unique_data_to_insert.append(doc)\n",
    "        seen_comments.add(comment)\n",
    "\n",
    "dataset_data = {\n",
    "    \"chart_type\": f\"datasets\",\n",
    "    \"dataset\": unique_data_to_insert,\n",
    "}\n",
    "\n",
    "# Send the unique data to the queue\n",
    "await send_single_message(sender, dataset_data)\n",
    "\n",
    "# Close the sender and the client\n",
    "await sender.close()\n",
    "await servicebus_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
